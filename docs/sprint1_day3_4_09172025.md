# Sprint 1, Day 3-4 (09/17/2025) - Python Script Execution Debugging Log

This document details the debugging process for an issue where the Java application was unable to get a response from executing a Python script in a Docker container.

## 1. The Problem

The initial problem was that a `curl` request to the `/api/python/hello` endpoint, which triggers a Python script execution, resulted in no output and an HTTP 500 error.

The application logs showed a `PythonScriptException` with the message "Python script execution failed with exit code: 1", but the script output was empty.

## 2. Diagnostic Attempts

### Attempt 1: Direct `docker-compose exec` from Terminal
-   **Action:** We tried to run the exact command from the Java code directly in the terminal: `docker-compose exec golden-bridge-python /usr/local/bin/python3 /app/python-scripts/hello.py ManualTest`.
-   **Result:** No output, and an exit code of 1.
-   **Analysis:** This was a crucial step. It proved that the issue was not with the Java `ProcessBuilder` code, but with `docker-compose exec` itself or the container environment.

### Attempt 2: Simplify the `docker-compose exec` Command
-   **Action:** To see if *any* command would work, we tried a simpler command: `docker-compose exec golden-bridge-python ls -l /app`.
-   **Result:** No output.
-   **Analysis:** This confirmed that the problem was not specific to Python, but that `docker-compose exec` was failing to execute any command in the container.

### Attempt 3: Get an Interactive Shell with `docker-compose exec`
-   **Action:** We tried to get an interactive shell using `docker-compose exec golden-bridge-python /bin/bash` and `/bin/sh`.
-   **Result:** No output.
-   **Analysis:** This was a strong indicator that `docker-compose exec` was fundamentally broken for this container setup.

### Attempt 4: Restart Docker Containers
-   **Action:** We restarted the containers using `docker-compose down` and `docker-compose up -d`.
-   **Result:** The issue persisted. `docker-compose exec` still produced no output.
-   **Analysis:** This ruled out a transient state issue with the containers.

### Attempt 5: Bypass `docker-compose` and use `docker exec`
-   **Action:** We tried executing a command using `docker exec` directly, bypassing `docker-compose`: `docker exec golden-bridge-python ls -l /app`.
-   **Result:** Success! The command returned the directory listing for `/app`.
-   **Analysis:** This was the breakthrough. It isolated the problem to the `docker-compose` command itself on the user's machine, and not Docker Engine or the container.

## 3. Root Cause

The root cause was an issue with `docker-compose exec` on the host machine's environment, which prevented it from successfully executing commands in the running `golden-bridge-python` container. The reason for `docker-compose exec` failing while `docker exec` works is not fully clear, but it could be related to the specific version of `docker-compose`, its configuration, or how it handles TTY allocation in non-interactive sessions.

## 4. Solution

The solution was to modify the Java code to use `docker exec` instead of `docker-compose exec`.

-   **Action:** The `PythonScriptService.java` was updated to change the command from `["docker-compose", "exec", ...]` to `["docker", "exec", ...]`.
-   **Result:** After this change and restarting the application, the manual test of the `/api/python/hello` endpoint was successful.

---

## Architectural Refactor: From Process Execution to Service-Based API

### The Problem
The initial implementation using `ProcessBuilder` to call `docker exec` worked for basic script execution, but it proved impossible to test the timeout functionality reliably. The `process.waitFor()` method did not behave as expected when wrapping a `docker exec` command, and further debugging revealed issues with `docker-compose exec` on the host machine. This approach was deemed too brittle and environment-dependent.

### The Solution: A Service-Based Architecture
We made the decision to refactor the integration to a more robust and standard service-based architecture.

1.  **Python Service:** The `hello.py` script was converted into a Flask web server, running on port 5001 inside its Docker container.
2.  **Docker Configuration:** The `docker-compose.yml` was updated to expose port 5001 and to run the Flask server directly as its command.
3.  **Java Service:** The `GarminIntegrationService` was completely rewritten. Instead of `ProcessBuilder`, it now uses a Spring `RestTemplate` to make HTTP calls to the Python service.
4.  **Testing:** The old, brittle integration tests were replaced with a clean `@RestClientTest`. This test uses a `MockRestServiceServer` to mock the Python service, allowing for fast, reliable, and isolated testing of the Java code.

### The Result
This new architecture is not only more testable but also more robust, scalable, and easier to maintain, aligning with modern microservice best practices. The manual end-to-end test of the new architecture was successful.

## Garmin API Integration: Authentication (Login, Status, Logout)

### Implementation Details
Following the refactoring to a service-based architecture, the Garmin authentication features were implemented:

1.  **Python Service (`garmin_api.py`):**
    *   A new Flask application was created to encapsulate Garmin API interactions.
    *   `/garmin/login` (POST): Accepts username and password, uses `garminconnect` to log in, and stores the `Garmin` client instance globally to maintain session state. Returns success/error status.
    *   `/garmin/status` (GET): Returns the current login status (logged_in/logged_out) and the username if logged in.
    *   `/garmin/logout` (POST): Calls the `garminconnect` logout method and clears the stored client instance.

2.  **Java Service (`GarminIntegrationService.java`):**
    *   New DTOs (`GarminLoginRequest`, `GarminLoginResponse`, `GarminStatusResponse`, `GarminLogoutResponse`) were created for clear data transfer.
    *   `loginToGarmin(GarminLoginRequest)`: Calls the Python `/garmin/login` endpoint.
    *   `getGarminStatus()`: Calls the Python `/garmin/status` endpoint.
    *   `logoutFromGarmin()`: Calls the Python `/garmin/logout` endpoint.

3.  **Java Controller (`AuthController.java`):**
    *   New endpoints were added to expose the Garmin functionality to the frontend:
        *   `POST /api/auth/garmin/login`
        *   `GET /api/auth/garmin/status`
        *   `POST /api/auth/garmin/logout`

### Manual Testing & Verification
The implemented features were manually tested end-to-end:
*   Successful login to Garmin Connect via the Java application.
*   Verification of persistent login status using the status endpoint.
*   Successful logout from Garmin Connect.

---

## Debugging and Refactoring the `GarminIntegrationServiceTest`

### Initial Problem: `RestTemplate` Bean Not Found
*   **Error:** `Error creating bean with name 'garminIntegrationService': Unsatisfied dependency expressed through constructor parameter 0: No qualifying bean of type 'org.springframework.web.client.RestTemplate' available.`
*   **Cause:** When switching `GarminIntegrationServiceTest` to `@RestClientTest`, the test context was too narrow. It didn't automatically load `AppConfig.java`, where our `RestTemplate` bean was defined.
*   **Attempted Fix:** Added `@Import(AppConfig.class)` to `GarminIntegrationServiceTest`.
*   **Resulting Error:** `java.lang.IllegalStateException: Unable to use auto-configured MockRestServiceServer since a mock server customizer has not been bound to a RestTemplate or RestClient`. This indicated that while `RestTemplate` was now available, `MockRestServiceServer` didn't know how to mock it because it wasn't created with the test's `RestTemplateBuilder`.

### Second Problem: `RestTemplate` Not Mockable by `@RestClientTest`
*   **Error:** `Unable to use auto-configured MockRestServiceServer since a mock server customizer has not been bound to a RestTemplate or RestClient`.
*   **Cause:** Our `AppConfig` was creating `new RestTemplate()` directly. `@RestClientTest` expects the `RestTemplate` to be built using its auto-configured `RestTemplateBuilder` so it can inject its mocking capabilities.
*   **Attempted Fix:** Modified `AppConfig.java` to accept `RestTemplateBuilder` and use `builder.build()` to create the `RestTemplate` bean.
*   **Resulting Error:** `java.lang.IllegalStateException: Unable to find a @SpringBootConfiguration by searching packages upwards from the test`. This was a misleading error. It suggested the test couldn't find the main application context, which is usually needed for `@SpringBootTest`.

### Third Problem: `@RestClientTest` Context Loading Issues
*   **Error:** `Unable to find a @SpringBootConfiguration by searching packages upwards from the test`.
*   **Cause:** This error was a red herring. While `@RestClientTest` is a slice test, it was still trying to locate a `@SpringBootConfiguration` for some reason, and our package structure or some other subtle configuration was preventing it from doing so automatically. Attempts to explicitly define the `classes` for `@RestClientTest` failed because that annotation doesn't support it.
*   **Attempted Fixes:**
    *   Added `classes = GoldenBridgeApplication.class` to `@RestClientTest` (failed compilation).
    *   Removed `classes` attribute (resulted in the same `Unable to find @SpringBootConfiguration` error).

### Final Solution: Switching to `@SpringBootTest` with `@MockBean`
*   **Solution:** We abandoned `@RestClientTest` for `GarminIntegrationServiceTest` and switched to `@SpringBootTest` combined with `@MockBean RestTemplate`.
    *   `@SpringBootTest` loads the full application context, ensuring all beans (like `RestTemplate` from `AppConfig`) are available.
    *   `@MockBean RestTemplate` replaces the real `RestTemplate` with a Mockito mock, allowing us to control its behavior for each test case using `Mockito.when()`.
*   **Robustness:** This is a highly robust and widely accepted testing pattern for services that interact with external APIs via `RestTemplate` in Spring Boot.
    *   **Reliability:** Tests are isolated from actual external services, preventing network issues or external API changes from breaking tests.
    *   **Control:** We have granular control over the `RestTemplate`'s responses, allowing us to simulate various scenarios (success, different error codes, specific data).
    *   **Clarity:** The test code clearly defines the expected HTTP interactions.
    *   **Maintainability:** It's a standard pattern, making the tests easier to understand and maintain by other developers.